{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a24c575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai) (3.9.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.20->openai) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.20->openai) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.20->openai) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.20->openai) (2022.9.24)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->openai) (21.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sec\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52e64c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.7.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-generativeai) (0.6.6)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-generativeai) (2.19.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-generativeai) (2.143.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-generativeai) (2.27.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-generativeai) (3.19.6)\n",
      "Requirement already satisfied: pydantic in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-generativeai) (2.7.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-generativeai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-ai-generativelanguage==0.6.6->google-generativeai) (1.24.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-api-core->google-generativeai) (1.63.2)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.64.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.62.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-4.25.8-cp39-cp39-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2022.9.24)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (2.4.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic->google-generativeai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pydantic->google-generativeai) (2.18.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sec\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Downloading protobuf-4.25.8-cp39-cp39-win_amd64.whl (413 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "Successfully installed protobuf-4.25.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\SEC\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\google\\~-otobuf'.\n",
      "  You can safely remove it manually.\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\sec\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mysql-connector-python 8.2.0 requires protobuf<=4.21.12,>=4.21.1, but you have protobuf 4.25.8 which is incompatible.\n",
      "paddlepaddle 2.6.2 requires protobuf<=3.20.2,>=3.1.0; platform_system == \"Windows\", but you have protobuf 4.25.8 which is incompatible.\n",
      "streamlit 1.22.0 requires protobuf<4,>=3.12, but you have protobuf 4.25.8 which is incompatible.\n",
      "tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.25.8 which is incompatible.\n",
      "tensorflow-intel 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.8 which is incompatible.\n",
      "tf2onnx 1.16.1 requires protobuf~=3.20, but you have protobuf 4.25.8 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install google-generativeai python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d9140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Text extraction successful without OCR.\n",
      "[INFO] PDF split into 1 chunks.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4500226a2abe4f36b0b4af19da6547b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top relevant PDF passages ---\n",
      "\n",
      "Chunk #1 (sim=0.29):\n",
      "Yuvaraj S Machine Learning Engineer — Software Engineer (cid:131) +91 9384137766 # ai.yuvaraj21@gmail.com (cid:239) LinkedIn § GitHub (cid:128) Portfolio Aspiring Machine Learning Engineer with a passion for building scalable AI solutions and real-time applications. Technical Skills Languages: Python, JavaScript, HTML, CSS ML/AI: TensorFlow, PyTorch, Scikit-learn, OpenCV, BERT, Neural Networks, Computer Vision, NLP Web/Backend: Django, REST API, MongoDB Tools/Cloud: Git, AWS, Jupyter, Postman, CI/CD Experience Machine Learning Engineer July 2024 – Present Techso IT LLC, USA – Developed real-ti\n",
      "\n",
      "Final Answer:\n",
      " The provided text describes several projects undertaken by Yuvaraj S.  There's not a single, overarching \"project,\" but rather multiple projects detailed under \"Key Projects\" and within his work experience.  Here's a summary:\n",
      "\n",
      "**Key Projects:**\n",
      "\n",
      "* **Library Access Control System:** This project involved creating a Django REST API backend with MySQL for user management and access logging.  It also included a real-time video processing pipeline using OpenCV for multi-camera integration and an admin dashboard with analytics showing a 25% improvement in access management efficiency.  The system used Python, TensorFlow, Django, Android Studio, and AndroidX.\n",
      "\n",
      "* **FlipIQ AI E-Commerce Platform:** This project focused on building an e-commerce platform using PyTorch for computer vision (product similarity matching with 88% accuracy using ResNet50). The frontend was built with HTML, CSS, and JavaScript, and the backend used the Django framework, handling over 1000 concurrent users.\n",
      "\n",
      "* **EchoFlow Audio Analysis:** This project involved designing a real-time audio transcription and sentiment analysis pipeline (95% accuracy for transcription and 87% for sentiment analysis using BERT).  A Django API was developed for audio data storage and retrieval, along with an automated reporting system analyzing 10,000+ audio files daily.  TensorFlow and BERT were key technologies used.\n",
      "\n",
      "\n",
      "**Projects within Work Experience:**  These are less fully detailed but show further examples of Yuvaraj's work:\n",
      "\n",
      "* **Real-time audio transcription models (Techso IT LLC):**  Developed models with 80% accuracy using TensorFlow and Python.\n",
      "* **NLP summarization systems (Techso IT LLC):**  Reduced content processing time by 40% using BERT.\n",
      "* **Flipkart Robotic Challenge:** Integrated 5 AI models into a website and developed RESTful APIs with sub-100ms latency.\n",
      "* **Amazon ML Challenge:** Built NLP models for product attribute extraction (78% accuracy) and implemented BERT-based feature extraction for e-commerce categorization (15% improvement).\n",
      "\n",
      "To get more specific details about a particular project, you'll need to specify which one you're interested in.\n",
      "\n",
      "--- Top relevant PDF passages ---\n",
      "\n",
      "Chunk #1 (sim=0.11):\n",
      "Yuvaraj S Machine Learning Engineer — Software Engineer (cid:131) +91 9384137766 # ai.yuvaraj21@gmail.com (cid:239) LinkedIn § GitHub (cid:128) Portfolio Aspiring Machine Learning Engineer with a passion for building scalable AI solutions and real-time applications. Technical Skills Languages: Python, JavaScript, HTML, CSS ML/AI: TensorFlow, PyTorch, Scikit-learn, OpenCV, BERT, Neural Networks, Computer Vision, NLP Web/Backend: Django, REST API, MongoDB Tools/Cloud: Git, AWS, Jupyter, Postman, CI/CD Experience Machine Learning Engineer July 2024 – Present Techso IT LLC, USA – Developed real-ti\n",
      "\n",
      "Final Answer:\n",
      " Based on the provided text, Yuvaraj S is an aspiring Machine Learning Engineer.  Their contact information includes their phone number (+91 9384137766) and email address (ai.yuvaraj21@gmail.com).  They also have LinkedIn and GitHub profiles.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "# ---- 1. PDF Extraction (OCR fallback) ----\n",
    "def extract_text_pdf_with_ocr(pdf_path, dpi=200):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            pg_text = page.extract_text()\n",
    "            if pg_text:\n",
    "                text += pg_text + \"\\n\"\n",
    "    if not text.strip():\n",
    "        print(\"[INFO] No selectable text—using OCR fallback.\")\n",
    "        images = convert_from_path(pdf_path, dpi=dpi)\n",
    "        for image in images:\n",
    "            text += pytesseract.image_to_string(image) + \"\\n\"\n",
    "    else:\n",
    "        print(\"[INFO] Text extraction successful without OCR.\")\n",
    "    return text\n",
    "\n",
    "# ---- 2. Chunk text for RAG ----\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        if i + chunk_size >= len(words):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "# ---- 3. Embedding ----\n",
    "def embed_chunks(chunks, model_name='all-MiniLM-L6-v2'):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(chunks, convert_to_tensor=False, show_progress_bar=True)\n",
    "    return model, embeddings\n",
    "\n",
    "# ---- 4. Query & Retrieval ----\n",
    "def retrieve_top_k(query, model, chunk_embeddings, chunks, k=3):\n",
    "    query_embed = model.encode(query, convert_to_tensor=False)\n",
    "    sims = cosine_similarity([query_embed], chunk_embeddings)[0]\n",
    "    top_idx = sims.argsort()[::-1][:k]\n",
    "    return [(chunks[i], sims[i]) for i in top_idx]\n",
    "\n",
    "# ---- 5. Gemini LLM Integration (Latest SDK) ----\n",
    "def generate_answer_gemini(\n",
    "    client,  # pass the client object!\n",
    "    question,\n",
    "    retrieved_chunks,\n",
    "    use_outside_knowledge=False,\n",
    "    model_name=\"models/gemini-1.5-flash-latest\"  # Use what is in client.models.list()\n",
    "):\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Context {i+1}:\\n{chunk[0] if isinstance(chunk, tuple) else chunk}\"\n",
    "        for i, chunk in enumerate(retrieved_chunks)\n",
    "    ])\n",
    "    if use_outside_knowledge:\n",
    "        sys_prompt = (\n",
    "            \"You are a helpful assistant. Answer the user's question using the provided PDF context. \"\n",
    "            \"You MAY also include helpful outside knowledge, but cite the PDF if it has the answer.\"\n",
    "        )\n",
    "    else:\n",
    "        sys_prompt = (\n",
    "            \"You are a helpful assistant. Answer using ONLY the PDF context below. \"\n",
    "            \"If the answer isn't present, say so (do NOT add outside knowledge).\"\n",
    "        )\n",
    "    prompt = f\"\"\"{sys_prompt}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=prompt\n",
    "    )\n",
    "    return response.text.strip()\n",
    "\n",
    "# ---- 6. Main Routine ----\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv()\n",
    "    # IMPORTANT: Set GOOGLE_API_KEY (or GEMINI_API_KEY) in your .env!\n",
    "    client = genai.Client()  # Reads the key from environment automatically\n",
    "\n",
    "    # --------- User Input Section ------------\n",
    "    PDF_PATH = \"ML_Resume.pdf\"\n",
    "    if not os.path.exists(PDF_PATH):\n",
    "        PDF_PATH = input(\"Enter your PDF file name/path: \").strip()\n",
    "\n",
    "    full_text = extract_text_pdf_with_ocr(PDF_PATH)\n",
    "    chunks = chunk_text(full_text, chunk_size=500, overlap=50)\n",
    "    print(f\"[INFO] PDF split into {len(chunks)} chunks.\")\n",
    "\n",
    "    embed_model, chunk_embeddings = embed_chunks(chunks)\n",
    "\n",
    "    while True:\n",
    "        user_question = input(\"\\nAsk a question about your PDF (or 'exit'): \").strip()\n",
    "        if user_question.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        top_chunks = retrieve_top_k(user_question, embed_model, chunk_embeddings, chunks, k=3)\n",
    "        print(\"\\n--- Top relevant PDF passages ---\")\n",
    "        for idx, (chunk, sim) in enumerate(top_chunks, 1):\n",
    "            print(f\"\\nChunk #{idx} (sim={sim:.2f}):\\n{chunk[:600]}\")\n",
    "\n",
    "        use_extra = input(\"\\nAllow outside knowledge in answer? (y/n): \").strip().lower() == \"y\"\n",
    "        answer = generate_answer_gemini(client, user_question, top_chunks, use_outside_knowledge=use_extra)\n",
    "        print(\"\\nFinal Answer:\\n\", answer)\n",
    "\n",
    "    print(\"\\n[Done]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f86a9d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS has 1 segments/chunks.\n",
      "Chunk 0: Yuvaraj S Machine Learning Engineer — Software Engineer (cid:131) +91 9384137766 # ai.yuvaraj21@gmai...\n",
      "Top matches for first chunk: ['Yuvaraj S Machine Learning Engineer — Software Engineer (cid:131) +91 9384137766 # ai.yuvaraj21@gmail.com (cid:239) LinkedIn § GitHub (cid:128) Portfolio Aspiring Machine Learning Engineer with a passion for building scalable AI solutions and real-time applications. Technical Skills Languages: Python, JavaScript, HTML, CSS ML/AI: TensorFlow, PyTorch, Scikit-learn, OpenCV, BERT, Neural Networks, Computer Vision, NLP Web/Backend: Django, REST API, MongoDB Tools/Cloud: Git, AWS, Jupyter, Postman, CI/CD Experience Machine Learning Engineer July 2024 – Present Techso IT LLC, USA – Developed real-time audio transcription models with 80% accuracy using TensorFlow and Python – Built NLP summarization systems reducing content processing time by 40% with BERT implementation – Integrated ML pipelines with Django REST APIs for production deployment serving AI/Software Developer Oct 2024 Flipkart Robotic Challenge (National) – Integrated 5 AI models in a Website – Developed RESTful APIs with sub-100ms latency for real-time inference and data processing ML Developer Sep 2024 Amazon ML Challenge (National) – Built NLP models for product attribute extraction with 78% accuracy using transformer architectures – Implemented BERT-based feature extraction for e-commerce categorization with 15% improvement Key Projects Library Access Control System | Python, TensorFlow, Django, Android Studio, AndroidX Nov 2024 – Developed Django REST API backend with MySQL for user management and access logging – Implemented real-time video processing pipeline with OpenCV for multi-camera integration – Created admin dashboard with analytics showing 25% improvement in access management efficiency FlipIQ AI E-Commerce Platform | PyTorch, HTML, CSS, JavaScript, Django Oct 2024 – Implemented computer vision for product similarity matching with 88% accuracy using ResNet50 – Built HTML, CSS, JavaScript frontend with Django Framework backend handling 1000+ concurrent users EchoFlow Audio Analysis | TensorFlow, BERT, Django Jul 2024 – Designed real-time audio transcription and sentiment analysis pipeline with 95% accuracy – Built BERT-based sentiment analysis with 87% accuracy for customer feedback classification – Developed Django API for audio data storage and retrieval – Created automated reporting system generating insights from 10K+ audio files daily Education B.Tech in Artificial Intelligence and Machine Learning Expected: May 2026 Saveetha Engineering College Chennai, India – Relevant Coursework: Deep Learning, Computer Vision, Natural Language Processing, Data Structures, Web Development - Django Framework – CGPA: 8.3/10 — Dean’s List recipient for academic excellence in AI/ML subjects Certifications AWS Certified AI Practitioner (Feb 2025) — NVIDIA LLM Customization (May 2025) Oracle Cloud AI Foundations (Jan 2025) — Deep Learning Specialization (Nov 2023) TensorFlow Developer Certificate (Dec 2024)']\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load index and chunks\n",
    "index = faiss.read_index(\"ML_Resume.pdf.faiss\")\n",
    "with open(\"ML_Resume.pdf.pkl\", \"rb\") as f:\n",
    "    chunks = pickle.load(f)\n",
    "\n",
    "print(f\"FAISS has {index.ntotal} segments/chunks.\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}: {chunk[:100]}...\")  # Show first 100 chars\n",
    "\n",
    "# Show nearest neighbor for first chunk\n",
    "vec = index.reconstruct(0)\n",
    "D, I = index.search(np.array([vec]), k=3)\n",
    "# Filter out invalid indices (-1) before accessing chunks\n",
    "valid_indices = [int(j) for j in I[0] if j != -1]\n",
    "print(\"Top matches for first chunk:\", [chunks[j] for j in valid_indices])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
